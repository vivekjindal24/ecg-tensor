{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "307eef2cccb566c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Environment Setup\n",
   "id": "61cf2f534128517b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T04:40:14.283041Z",
     "start_time": "2025-11-10T04:40:02.222240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Environment Setup (robust torch import)\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Attempt torch import with graceful fallback message for DLL issues\n",
    "try:\n",
    "    import torch  # noqa: E402\n",
    "except OSError as e:\n",
    "    print(\"Torch import failed (likely CUDA DLL issue). Falling back instructions.\")\n",
    "    print(\"Original error:\\n\", e)\n",
    "    print(\"If you lack proper NVIDIA drivers or want CPU-only, reinstall with:\\n  pip install --force-reinstall --no-cache-dir torch --index-url https://download.pytorch.org/whl/cpu\")\n",
    "    # Retry CPU-only import if already installed\n",
    "    try:\n",
    "        import importlib\n",
    "        torch = importlib.import_module('torch')  # noqa: F401\n",
    "    except Exception:\n",
    "        raise\n",
    "\n",
    "import wfdb\n",
    "from scipy.signal import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import fastapi  # ensure core library is available\n",
    "import uvicorn  # ensure core library is available\n",
    "\n",
    "_ = (os, sys, fastapi, uvicorn)\n",
    "\n",
    "ROOT = Path('.')\n",
    "ARTIFACTS_DIR = ROOT / 'artifacts'\n",
    "MODELS_DIR = ARTIFACTS_DIR / 'models'\n",
    "PROCESSED_DIR = ARTIFACTS_DIR / 'processed'\n",
    "MLFLOW_DIR = ARTIFACTS_DIR / 'mlflow'\n",
    "DATASET_DIR = ROOT / 'dataset'\n",
    "FIGURES_DIR = ROOT / 'figures'\n",
    "LOGS_DIR = ROOT / 'logs'\n",
    "\n",
    "for d in [ARTIFACTS_DIR, MODELS_DIR, PROCESSED_DIR, MLFLOW_DIR, DATASET_DIR, FIGURES_DIR, LOGS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cuda_available = torch.cuda.is_available() if hasattr(torch, 'cuda') else False\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "if cuda_available:\n",
    "    try:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"GPU name unavailable: {e}\")\n",
    "\n",
    "print(\"Paths configured:\")\n",
    "print(f\"- ROOT:          {ROOT.resolve().as_posix()}\")\n",
    "print(f\"- DATASET_DIR:   {DATASET_DIR.resolve().as_posix()}\")\n",
    "print(f\"- ARTIFACTS_DIR: {ARTIFACTS_DIR.resolve().as_posix()}\")\n",
    "print(f\"  - MODELS_DIR:   {MODELS_DIR.resolve().as_posix()}\")\n",
    "print(f\"  - PROCESSED_DIR:{PROCESSED_DIR.resolve().as_posix()}\")\n",
    "print(f\"  - MLFLOW_DIR:   {MLFLOW_DIR.resolve().as_posix()}\")\n",
    "print(f\"- FIGURES_DIR:   {FIGURES_DIR.resolve().as_posix()}\")\n"
   ],
   "id": "80471af419e74588",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ecg-research\\.venv\\Lib\\site-packages\\mlflow\\utils\\requirements_utils.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources  # noqa: TID251\n",
      "D:\\ecg-research\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:383: UserWarning: Valid config keys have changed in V2:\n",
      "* 'schema_extra' has been renamed to 'json_schema_extra'\n",
      "  warnings.warn(message, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 2050\n",
      "Paths configured:\n",
      "- ROOT:          D:/ecg-research\n",
      "- DATASET_DIR:   D:/ecg-research/dataset\n",
      "- ARTIFACTS_DIR: D:/ecg-research/artifacts\n",
      "  - MODELS_DIR:   D:/ecg-research/artifacts/models\n",
      "  - PROCESSED_DIR:D:/ecg-research/artifacts/processed\n",
      "  - MLFLOW_DIR:   D:/ecg-research/artifacts/mlflow\n",
      "- FIGURES_DIR:   D:/ecg-research/figures\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing: Load, Normalize, and Split ECG Datasets\n",
   "id": "b74dddfec163692b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-10T10:22:56.362809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preprocessing (robust to out-of-order execution)\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import torch  # ensure torch is available here as well\n",
    "from pathlib import Path as _PathGuard\n",
    "\n",
    "# Safety: rehydrate core paths if this cell runs before Environment Setup\n",
    "try:\n",
    "    ROOT\n",
    "except NameError:\n",
    "    ROOT = _PathGuard('.')\n",
    "\n",
    "ARTIFACTS_DIR = ARTIFACTS_DIR if 'ARTIFACTS_DIR' in globals() else ROOT / 'artifacts'\n",
    "PROCESSED_DIR = PROCESSED_DIR if 'PROCESSED_DIR' in globals() else ARTIFACTS_DIR / 'processed'\n",
    "DATASET_DIR = DATASET_DIR if 'DATASET_DIR' in globals() else ROOT / 'dataset'\n",
    "FIGURES_DIR = FIGURES_DIR if 'FIGURES_DIR' in globals() else ROOT / 'figures'\n",
    "LOGS_DIR = LOGS_DIR if 'LOGS_DIR' in globals() else ROOT / 'logs'\n",
    "for _d in [ARTIFACTS_DIR, PROCESSED_DIR, DATASET_DIR, FIGURES_DIR, LOGS_DIR]:\n",
    "    _PathGuard(_d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TARGET_FS = 500\n",
    "TARGET_SAMPLES = 5000\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Unified label mapping setup\n",
    "UNIFIED_CSV = LOGS_DIR / 'unified_label_mapping.csv'\n",
    "LABEL_ORDER = ['MI', 'AF', 'BBB', 'NORM', 'OTHER']\n",
    "LABEL_TO_INT: Dict[str, int] = {name: idx for idx, name in enumerate(LABEL_ORDER)}\n",
    "\n",
    "# Build a robust mapping index from unified CSV\n",
    "mapping_index: Dict[str, Dict[str, str]] = {}\n",
    "if UNIFIED_CSV.exists():\n",
    "    umap_df = pd.read_csv(UNIFIED_CSV, dtype=str)\n",
    "    # Normalize columns\n",
    "    umap_df.columns = [c.strip() for c in umap_df.columns]\n",
    "    required_cols = {'dataset', 'record_id', 'mapped_label'}\n",
    "    missing = required_cols - set(umap_df.columns)\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Unified mapping missing columns: {missing}\")\n",
    "    for _, row in umap_df.iterrows():\n",
    "        ds = str(row['dataset']).strip()\n",
    "        rid = str(row['record_id']).strip()\n",
    "        lab = str(row['mapped_label']).strip().upper()\n",
    "        if not ds or not rid:\n",
    "            continue\n",
    "        if ds not in mapping_index:\n",
    "            mapping_index[ds] = {}\n",
    "        # Register multiple key variants for robust lookup\n",
    "        rid_norm = rid.replace('\\\\', '/').strip('/')\n",
    "        mapping_index[ds][rid_norm] = lab\n",
    "        try:\n",
    "            p = Path(rid_norm)\n",
    "            mapping_index[ds][p.name] = lab\n",
    "            if len(p.parts) >= 2:\n",
    "                mapping_index[ds]['/'.join(p.parts[-2:])] = lab\n",
    "            # If record_id already includes dataset name prefix, also register without it\n",
    "            if len(p.parts) >= 2 and p.parts[0] == ds:\n",
    "                mapping_index[ds]['/'.join(p.parts[1:])] = lab\n",
    "        except Exception:\n",
    "            pass\n",
    "else:\n",
    "    print(f\"WARNING: {UNIFIED_CSV.as_posix()} not found. Unmapped records will default to OTHER.\")\n",
    "\n",
    "print(\"Scanning dataset directory for WFDB headers (.hea)...\")\n",
    "hea_files = sorted(DATASET_DIR.rglob('*.hea'))\n",
    "\n",
    "# Optional fallback to .mat files if no WFDB records are found (helps common datasets)\n",
    "mat_files = sorted(DATASET_DIR.rglob('*.mat')) if not hea_files else []\n",
    "\n",
    "if not hea_files and not mat_files:\n",
    "    raise RuntimeError(f\"No supported ECG files found under {DATASET_DIR} (expected .hea or .mat)\")\n",
    "\n",
    "\n",
    "def zscore_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x, dtype=np.float32)\n",
    "    m = float(x.mean())\n",
    "    s = float(x.std())\n",
    "    if s < 1e-8:\n",
    "        s = 1.0\n",
    "    return (x - m) / s\n",
    "\n",
    "\n",
    "def pad_or_truncate(x: np.ndarray, length: int) -> np.ndarray:\n",
    "    if x.size < length:\n",
    "        return np.pad(x, (0, length - x.size), mode='constant')\n",
    "    if x.size > length:\n",
    "        return x[:length]\n",
    "    return x\n",
    "\n",
    "\n",
    "def _wfdb_read(record_header_path: Path) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Read a WFDB record given a .hea file path. Returns (1D_signal, fs).\"\"\"\n",
    "    record_path = record_header_path.with_suffix('')  # drop extension\n",
    "    try:\n",
    "        sig, fields = wfdb.rdsamp(str(record_path))\n",
    "        fs = float(fields.get('fs', TARGET_FS))\n",
    "        # sig shape: (N, channels) -> select first channel\n",
    "        if sig.ndim == 2:\n",
    "            sig_1d = sig[:, 0]\n",
    "        else:\n",
    "            sig_1d = sig.reshape(-1)\n",
    "        return sig_1d.astype(np.float32), fs\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"WFDB read failed for {record_header_path.name}: {e}\")\n",
    "\n",
    "\n",
    "def _mat_read(mat_path: Path) -> Tuple[np.ndarray, Optional[float]]:\n",
    "    \"\"\"Read a MATLAB .mat ECG file. Heuristics for common key names. fs may be unknown.\"\"\"\n",
    "    mat = loadmat(mat_path)\n",
    "    # Common keys: 'val' (WFDB-exported), 'data', 'signal'\n",
    "    for key in ('val', 'data', 'signal'):\n",
    "        if key in mat:\n",
    "            arr = np.asarray(mat[key])\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError(f\"Unexpected MAT structure for {mat_path.name}\")\n",
    "    # arr could be (n_leads, n) or (1, n)\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[0] > 1:\n",
    "            arr = arr[0]\n",
    "        else:\n",
    "            arr = arr.reshape(-1)\n",
    "    elif arr.ndim > 2:\n",
    "        arr = arr.reshape(-1)\n",
    "    arr = np.asarray(arr, dtype=np.float32)\n",
    "    fs = None  # Unknown without sidecar metadata\n",
    "    return arr, fs\n",
    "\n",
    "\n",
    "def _lookup_mapped_label(path: Path) -> str:\n",
    "    \"\"\"Lookup mapped label for a record path using unified mapping. Defaults to OTHER if not found.\"\"\"\n",
    "    rel = path.relative_to(DATASET_DIR).with_suffix('')\n",
    "    parts = rel.parts\n",
    "    if not parts:\n",
    "        return 'OTHER'\n",
    "    ds = parts[0]\n",
    "    # Build candidate keys\n",
    "    key_full = rel.as_posix()\n",
    "    key_wo_ds = '/'.join(parts[1:]) if len(parts) > 1 else ''\n",
    "    key_last2 = '/'.join(parts[-2:]) if len(parts) >= 2 else ''\n",
    "    key_name = rel.name\n",
    "    candidates = [key_full, key_wo_ds, key_last2, key_name]\n",
    "    index = mapping_index.get(ds, {})\n",
    "    for k in candidates:\n",
    "        if k and k in index:\n",
    "            lab = index[k]\n",
    "            return lab if lab in LABEL_TO_INT else 'OTHER'\n",
    "    # Special-case CinC where IDs may omit training/validation folder\n",
    "    if ds == 'CinC_2017_AFDB':\n",
    "        # Also try dropping potential 'training/' or 'validation/'\n",
    "        if len(parts) >= 3 and parts[1] in {'training', 'validation', 'test'}:\n",
    "            alt = '/'.join(parts[2:])\n",
    "            if alt in index:\n",
    "                lab = index[alt]\n",
    "                return lab if lab in LABEL_TO_INT else 'OTHER'\n",
    "    return 'OTHER'\n",
    "\n",
    "\n",
    "def _resample_if_needed(x: np.ndarray, fs: Optional[float]) -> np.ndarray:\n",
    "    if fs is None or np.isclose(fs, TARGET_FS):\n",
    "        return x\n",
    "    # Compute new length maintaining duration\n",
    "    new_len = int(round(x.size * TARGET_FS / float(fs)))\n",
    "    return resample(x, new_len)\n",
    "\n",
    "\n",
    "signals: List[np.ndarray] = []\n",
    "labels_str: List[str] = []\n",
    "labels_int: List[int] = []\n",
    "\n",
    "if hea_files:\n",
    "    print(f\"Found {len(hea_files)} WFDB header(s). Processing...\")\n",
    "    skipped = 0\n",
    "    for hea in hea_files:\n",
    "        try:\n",
    "            raw, fs = _wfdb_read(hea)\n",
    "            raw = _resample_if_needed(raw, fs)\n",
    "            raw = zscore_normalize(raw)\n",
    "            raw = pad_or_truncate(raw, TARGET_SAMPLES)\n",
    "            lab = _lookup_mapped_label(hea)\n",
    "            signals.append(raw.astype(np.float32))\n",
    "            labels_str.append(lab)\n",
    "            labels_int.append(LABEL_TO_INT.get(lab, LABEL_TO_INT['OTHER']))\n",
    "        except Exception as e:\n",
    "            skipped += 1\n",
    "            print(f\"Skipping {hea.name}: {e}\")\n",
    "    if skipped:\n",
    "        print(f\"Skipped {skipped} header(s) due to read errors.\")\n",
    "else:\n",
    "    print(f\"No WFDB headers found. Falling back to .mat processing ({len(mat_files)} file(s)).\")\n",
    "    for m in mat_files:\n",
    "        try:\n",
    "            raw, fs = _mat_read(m)\n",
    "            raw = _resample_if_needed(raw, fs)\n",
    "            raw = zscore_normalize(raw)\n",
    "            raw = pad_or_truncate(raw, TARGET_SAMPLES)\n",
    "            lab = 'OTHER'  # no unified mapping for arbitrary .mat; default\n",
    "            signals.append(raw.astype(np.float32))\n",
    "            labels_str.append(lab)\n",
    "            labels_int.append(LABEL_TO_INT['OTHER'])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {m.name}: {e}\")\n",
    "\n",
    "if not signals:\n",
    "    raise RuntimeError(\"No valid signals after preprocessing. Check dataset format.\")\n",
    "\n",
    "signals_np = np.stack(signals)\n",
    "labels_np = np.asarray(labels_int, dtype=np.int64)\n",
    "print(f\"Retained {signals_np.shape[0]} signals after preprocessing.\")\n",
    "\n",
    "# Split using fixed label ints\n",
    "try:\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        signals_np, labels_np, test_size=0.30, random_state=42, stratify=labels_np\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp\n",
    "    )\n",
    "except ValueError:\n",
    "    # Fallback without stratification (e.g., too few samples per class)\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        signals_np, labels_np, test_size=0.30, random_state=42, stratify=None\n",
    "    )\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.50, random_state=42, stratify=None\n",
    "    )\n",
    "\n",
    "# Save splits with both legacy and new keys\n",
    "np.savez(PROCESSED_DIR / 'train.npz', x=X_train.astype(np.float32), y=y_train.astype(np.int64),\n",
    "         signal=X_train.astype(np.float32), label=y_train.astype(np.int64))\n",
    "np.savez(PROCESSED_DIR / 'val.npz',   x=X_val.astype(np.float32),   y=y_val.astype(np.int64),\n",
    "         signal=X_val.astype(np.float32),   label=y_val.astype(np.int64))\n",
    "np.savez(PROCESSED_DIR / 'test.npz',  x=X_test.astype(np.float32),  y=y_test.astype(np.int64),\n",
    "         signal=X_test.astype(np.float32),  label=y_test.astype(np.int64))\n",
    "\n",
    "# Save label name order for training/eval compatibility\n",
    "label_names = np.array(LABEL_ORDER, dtype=object)\n",
    "np.save(PROCESSED_DIR / 'labels.npy', label_names)\n",
    "\n",
    "# Save explicit label map\n",
    "label_map = {\n",
    "    'label_to_int': LABEL_TO_INT,\n",
    "    'int_to_label': {str(int(v)): k for k, v in LABEL_TO_INT.items()}\n",
    "}\n",
    "(PROCESSED_DIR / 'label_map.json').write_text(json.dumps(label_map, indent=2), encoding='utf-8')\n",
    "\n",
    "# Split metadata\n",
    "from collections import Counter as _Counter\n",
    "split_metadata = {\n",
    "    'timestamp': datetime.now(timezone.utc).isoformat(timespec='seconds'),\n",
    "    'counts': {'train': int(X_train.shape[0]), 'val': int(X_val.shape[0]), 'test': int(X_test.shape[0])},\n",
    "    'label_index': LABEL_TO_INT,\n",
    "    'label_counts': {int(k): int(v) for k, v in _Counter(labels_np.tolist()).items()}\n",
    "}\n",
    "(PROCESSED_DIR / 'splits.json').write_text(json.dumps(split_metadata, indent=2), encoding='utf-8')\n",
    "\n",
    "# Print final counts\n",
    "final_counts = _Counter(labels_np.tolist())\n",
    "final_counts_str = {str(k): int(v) for k, v in final_counts.items()}\n",
    "print(\"Final label counts (by index):\")\n",
    "for idx in range(len(LABEL_ORDER)):\n",
    "    cnt = int(final_counts_str.get(str(idx), 0))\n",
    "    print(f\"  {idx}={LABEL_ORDER[idx]}: {cnt}\")\n",
    "\n",
    "print('Preprocessing with unified labels completed successfully.')\n"
   ],
   "id": "5ab57c93abd886",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning dataset directory for WFDB headers (.hea)...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model Training and Experiment Logging\n",
   "id": "4bfe9bf92e5aa3a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T09:07:25.778186Z",
     "start_time": "2025-11-10T09:01:56.174973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model Training with MLflow\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, path: Path):\n",
    "        data = np.load(path)\n",
    "        self.x = data['x'].astype(np.float32)\n",
    "        self.y = data['y'].astype(np.int64)\n",
    "    def __len__(self) -> int:\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, idx: int):\n",
    "        sig = self.x[idx]  # shape: (5000,)\n",
    "        x = torch.from_numpy(sig).float().unsqueeze(0).unsqueeze(0)  # (1,1,5000)\n",
    "        y = int(self.y[idx])\n",
    "        return x, y\n",
    "\n",
    "class SmallNet(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=(1, 7), padding=(0, 3)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((1, 2)),\n",
    "            nn.Conv2d(16, 32, kernel_size=(1, 5), padding=(0, 2)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((1, 2))\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 1, 1, TARGET_SAMPLES)\n",
    "            flattened = self.features(dummy).view(1, -1).size(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.classifier(self.features(x))\n",
    "\n",
    "# Load splits\n",
    "train_split = PROCESSED_DIR / 'train.npz'\n",
    "val_split = PROCESSED_DIR / 'val.npz'\n",
    "label_file = PROCESSED_DIR / 'labels.npy'\n",
    "if not train_split.exists():\n",
    "    raise FileNotFoundError('Missing train.npz. Run the Preprocessing cell first.')\n",
    "\n",
    "train_ds = NpzDataset(train_split)\n",
    "val_ds = NpzDataset(val_split)\n",
    "label_names = np.load(label_file, allow_pickle=True).tolist()\n",
    "num_classes = len(label_names)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 8\n",
    "learning_rate = 1e-3\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "model = SmallNet(num_classes=num_classes).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# MLflow\n",
    "tracking_uri = f\"sqlite:///{(MLFLOW_DIR / 'mlflow.db').as_posix()}\"\n",
    "mlflow.set_tracking_uri(tracking_uri)\n",
    "mlflow.set_experiment('ECG_Tensor_Research')\n",
    "\n",
    "\n",
    "def evaluate(loader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            # compute correct predictions via numpy to avoid static analysis confusion\n",
    "            correct += int(np.count_nonzero(preds.cpu().numpy() == yb.cpu().numpy()))\n",
    "            total += xb.size(0)\n",
    "    return (total_loss / max(total, 1)), (correct / max(total, 1))\n",
    "\n",
    "history = []\n",
    "best_val_loss = float('inf')\n",
    "best_state = None\n",
    "best_epoch = -1\n",
    "\n",
    "with mlflow.start_run(run_name='notebook-training'):\n",
    "    mlflow.log_params({\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'architecture': 'SmallNet-2xConv2D'\n",
    "    })\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            yb = yb.to(DEVICE)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running += loss.item() * xb.size(0)\n",
    "        train_loss = running / len(train_ds)\n",
    "        val_loss, val_acc = evaluate(val_loader)\n",
    "        history.append({'epoch': epoch, 'train_loss': train_loss, 'val_loss': val_loss, 'val_acc': val_acc})\n",
    "        mlflow.log_metric('train_loss', train_loss, step=epoch)\n",
    "        mlflow.log_metric('val_loss', val_loss, step=epoch)\n",
    "        mlflow.log_metric('val_accuracy', val_acc, step=epoch)\n",
    "        print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_acc={val_acc:.3f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "\n",
    "    # Load best and save\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        print(f\"Loaded best model from epoch {best_epoch}\")\n",
    "    # Save checkpoint with label names for serving\n",
    "    best_model_path = MODELS_DIR / 'best_model.pt'\n",
    "    torch.save({\n",
    "        'state_dict': model.state_dict(),\n",
    "        'labels': label_names,\n",
    "        'config': {\n",
    "            'target_fs': TARGET_FS,\n",
    "            'target_samples': TARGET_SAMPLES,\n",
    "            'architecture': 'SmallNet-2xConv2D'\n",
    "        }\n",
    "    }, best_model_path)\n",
    "    mlflow.log_artifact(str(best_model_path), artifact_path='models')\n",
    "    # Save training history\n",
    "    history_path = MODELS_DIR / 'training_history.json'\n",
    "    history_path.write_text(json.dumps(history, indent=2), encoding='utf-8')\n",
    "    mlflow.log_artifact(str(history_path), artifact_path='history')\n",
    "\n",
    "print(\"Training finished.\")\n"
   ],
   "id": "834fdb2ff1951cec",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/10 14:32:19 INFO mlflow.tracking.fluent: Experiment with name 'ECG_Tensor_Research' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=5.2410 | val_loss=4.6069 | val_acc=0.111\n",
      "Epoch 02 | train_loss=4.6908 | val_loss=4.4625 | val_acc=0.119\n",
      "Epoch 03 | train_loss=4.5052 | val_loss=4.3770 | val_acc=0.123\n",
      "Epoch 04 | train_loss=4.4075 | val_loss=4.3645 | val_acc=0.124\n",
      "Epoch 05 | train_loss=4.3206 | val_loss=4.3288 | val_acc=0.124\n",
      "Epoch 06 | train_loss=4.2411 | val_loss=4.3419 | val_acc=0.126\n",
      "Epoch 07 | train_loss=4.1952 | val_loss=4.6939 | val_acc=0.114\n",
      "Epoch 08 | train_loss=4.1296 | val_loss=4.3063 | val_acc=0.147\n",
      "Loaded best model from epoch 8\n",
      "Training finished.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation: Metrics and Diagnostic Plots\n",
   "id": "1972bb81efe6bbdb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation and Visualization (force a dedicated MLflow run for test metrics)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay, roc_curve\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Load test split\n",
    "test_split = PROCESSED_DIR / 'test.npz'\n",
    "label_file = PROCESSED_DIR / 'labels.npy'\n",
    "if not test_split.exists():\n",
    "    raise FileNotFoundError('Missing test.npz. Run the Preprocessing cell first.')\n",
    "\n",
    "class _NPZ(Dataset):\n",
    "    def __init__(self, path: Path):\n",
    "        data = np.load(path)\n",
    "        self.x = data['x'].astype(np.float32)\n",
    "        self.y = data['y'].astype(np.int64)\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.from_numpy(self.x[i]).float().unsqueeze(0).unsqueeze(0)\n",
    "        y = int(self.y[i])\n",
    "        return x, y\n",
    "\n",
    "def _load_best_model() -> Tuple[torch.nn.Module, list]:\n",
    "    ckpt_path = MODELS_DIR / 'best_model.pt'\n",
    "    if not ckpt_path.exists():\n",
    "        raise FileNotFoundError('best_model.pt not found. Train the model first.')\n",
    "    payload = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    labels = payload.get('labels')\n",
    "    model = SmallNet(num_classes=len(labels))\n",
    "    model.load_state_dict(payload['state_dict'])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, labels\n",
    "\n",
    "model, label_names = _load_best_model()\n",
    "num_classes = len(label_names)\n",
    "\n",
    "test_ds = _NPZ(test_split)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "all_logits = []\n",
    "all_targets = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        logits = model(xb)\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        all_targets.append(yb.numpy())\n",
    "\n",
    "logits = np.concatenate(all_logits, axis=0)\n",
    "y_true = np.concatenate(all_targets, axis=0)\n",
    "y_prob = torch.softmax(torch.from_numpy(logits), dim=1).numpy()\n",
    "y_pred = y_prob.argmax(axis=1)\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "# F1 macro to handle multiclass\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# ROC-AUC handling for binary vs multiclass\n",
    "try:\n",
    "    if num_classes == 2:\n",
    "        y_true_bin = y_true\n",
    "        auc = roc_auc_score(y_true_bin, y_prob[:, 1])\n",
    "    else:\n",
    "        y_true_oh = label_binarize(y_true, classes=list(range(num_classes)))\n",
    "        auc = roc_auc_score(y_true_oh, y_prob, average='macro', multi_class='ovr')\n",
    "except Exception:\n",
    "    auc = float('nan')\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "\n",
    "# Plot ROC (macro)\n",
    "plt.figure(figsize=(6, 5))\n",
    "if num_classes == 2:\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob[:, 1])\n",
    "    plt.plot(fpr, tpr, label=f'AUC={auc:.3f}')\n",
    "else:\n",
    "    # Macro-average ROC\n",
    "    # Compute per-class ROC and average FPR grid\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1001)\n",
    "    tprs = []\n",
    "    for c in range(num_classes):\n",
    "        try:\n",
    "            y_true_c = np.asarray(y_true == c, dtype=int)\n",
    "            fpr_c, tpr_c, _ = roc_curve(y_true_c, y_prob[:, c])\n",
    "            tpr_interp = np.interp(fpr_grid, fpr_c, tpr_c)\n",
    "            tprs.append(tpr_interp)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if tprs:\n",
    "        mean_tpr = np.mean(tprs, axis=0)\n",
    "        plt.plot(fpr_grid, mean_tpr, label=f'Macro AUC={auc:.3f}')\n",
    "    else:\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='No ROC')\n",
    "plt.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "roc_path = FIGURES_DIR / 'roc.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(roc_path, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Plot confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "Disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names)\n",
    "Disp.plot(ax=ax, cmap='Blues', colorbar=False)\n",
    "plt.title('Confusion Matrix')\n",
    "cm_path = FIGURES_DIR / 'confusion_matrix.png'\n",
    "plt.tight_layout()\n",
    "plt.savefig(cm_path, dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Log to MLflow (if run is active, use current; else create a short one)\n",
    "try:\n",
    "    # End any previous active run cleanly if necessary\n",
    "    if mlflow.active_run():\n",
    "        mlflow.end_run()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "with mlflow.start_run(run_name='evaluation'):\n",
    "    mlflow.log_metric('test_accuracy', float(acc))\n",
    "    if not np.isnan(auc):\n",
    "        mlflow.log_metric('test_auroc', float(auc))\n",
    "    mlflow.log_metric('test_f1_macro', float(f1))\n",
    "    mlflow.log_artifact(str(roc_path), artifact_path='figures')\n",
    "    mlflow.log_artifact(str(cm_path), artifact_path='figures')\n",
    "\n",
    "print(\"Evaluation summary:\")\n",
    "print(f\"- Accuracy: {acc:.4f}\")\n",
    "print(f\"- AUROC:   {auc:.4f}\")\n",
    "print(f\"- F1:      {f1:.4f}\")\n"
   ],
   "id": "80430b105e839c66",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Serving via FastAPI\n",
   "id": "609a8766785eefb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Serving API (do not launch server from within the notebook)\n",
    "import io\n",
    "from fastapi import FastAPI, UploadFile, File\n",
    "from fastapi.responses import JSONResponse\n",
    "\n",
    "MODEL_CACHE = {}\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path: Optional[Path] = None):\n",
    "    if checkpoint_path is None:\n",
    "        # pick latest .pt in models dir\n",
    "        candidates = sorted(MODELS_DIR.glob('*.pt'), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not candidates:\n",
    "            raise FileNotFoundError('No checkpoint available. Train the model before serving.')\n",
    "        checkpoint_path = candidates[0]\n",
    "    payload = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    labels = payload.get('labels')\n",
    "    model = SmallNet(num_classes=len(labels))\n",
    "    model.load_state_dict(payload['state_dict'])\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, labels, checkpoint_path\n",
    "\n",
    "\n",
    "def prepare_for_inference(arr: np.ndarray) -> torch.Tensor:\n",
    "    arr = np.asarray(arr).astype(np.float32)\n",
    "    if arr.ndim > 1:\n",
    "        arr = arr.reshape(-1)\n",
    "    if arr.size != TARGET_SAMPLES:\n",
    "        arr = resample(arr, TARGET_SAMPLES)\n",
    "    # z-score\n",
    "    m = float(arr.mean())\n",
    "    s = float(arr.std()) or 1.0\n",
    "    arr = (arr - m) / s\n",
    "    return torch.from_numpy(arr).float().unsqueeze(0).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "\n",
    "def predict_signal(input_data: np.ndarray) -> dict:\n",
    "    if 'model' not in MODEL_CACHE:\n",
    "        model, labels, ckpt = load_checkpoint()\n",
    "        MODEL_CACHE['model'] = model\n",
    "        MODEL_CACHE['labels'] = labels\n",
    "        MODEL_CACHE['ckpt'] = ckpt\n",
    "        print(f\"Loaded checkpoint {ckpt.name} for serving.\")\n",
    "    model = MODEL_CACHE['model']\n",
    "    labels = MODEL_CACHE['labels']\n",
    "    xb = prepare_for_inference(input_data)\n",
    "    with torch.no_grad():\n",
    "        logits = model(xb)\n",
    "        probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "    top = int(np.argmax(probs))\n",
    "    return {\n",
    "        'predicted_class': labels[top],\n",
    "        'confidence': float(probs[top]),\n",
    "        'probabilities': {label: float(p) for label, p in zip(labels, probs)}\n",
    "    }\n",
    "\n",
    "app = FastAPI(title='ECG Tensor Inference API')\n",
    "\n",
    "@app.post('/predict')\n",
    "async def predict_endpoint(file: UploadFile = File(...)):\n",
    "    payload = await file.read()\n",
    "    buffer = io.BytesIO(payload)\n",
    "    suffix = Path(file.filename).suffix.lower()\n",
    "    if suffix == '.npy':\n",
    "        buffer.seek(0)\n",
    "        arr = np.load(buffer, allow_pickle=True)\n",
    "    else:  # assume CSV\n",
    "        buffer.seek(0)\n",
    "        arr = np.loadtxt(buffer, delimiter=',')\n",
    "    result = predict_signal(arr)\n",
    "    return JSONResponse(result)\n",
    "\n",
    "# Run this in terminal using uvicorn serve:app --reload\n"
   ],
   "id": "d27339240e7f6c46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "### 6. PowerShell Execution Instructions\n",
    "\n",
    "```powershell\n",
    "# (Optional) Create and activate a virtual environment\n",
    "python -m venv .venv\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# Install dependencies\n",
    "pip install -U pip\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Preprocessing (run the notebook cell or execute an equivalent script if you extract it)\n",
    "# Train (runs MLflow tracking to ./artifacts/mlflow/mlflow.db)\n",
    "# Evaluate and generate figures in ./figures/\n",
    "\n",
    "# Start MLflow UI in a separate terminal (optional)\n",
    "mlflow ui --backend-store-uri sqlite:///artifacts/mlflow/mlflow.db --default-artifact-root ./artifacts --host 127.0.0.1 --port 5000\n",
    "\n",
    "# Serve the API locally (from repository root)\n",
    "# NOTE: The app is defined in this notebook as variable `app`. If exported to a module named `serve.py`, you can run:\n",
    "# uvicorn serve:app --reload\n",
    "# Or, if using the notebook file/module name, adapt accordingly:\n",
    "# uvicorn ecg_tensor_pipeline:app --reload\n",
    "```\n"
   ],
   "id": "2e2907cc4baf3a27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ECG Research",
   "language": "python",
   "name": "ecg-research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
